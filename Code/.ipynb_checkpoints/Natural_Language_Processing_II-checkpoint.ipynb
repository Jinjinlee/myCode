{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using NLTK\n",
    "\n",
    "references:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    " - https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    " - http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open python and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora, and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into words, punctuation sysmbols, or segments\n",
    "   * Understand vocabulary of the text\n",
    "   * Extract features for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words and punctuation symbols\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Create document-to-term matrix (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text=\". \".join(news).lower()\n",
    "print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Sentence\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "   \n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print len(sentences)\n",
    "print sentences\n",
    "\n",
    "# what patterns can be used to segment \n",
    "# text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text\n",
    "\n",
    "# unigram tokenization pattern\n",
    "pattern=r'\\w+[\\-\\.]*\\w+'                          \n",
    "\n",
    "# get unigrams\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "bigrams=list(nltk.bigrams(tokens))\n",
    "print(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: A collocation is a sequence of words that occur together unusually often, e.g. red wine, United States, graduate students, go to school etc.\n",
    " - Question: how to find collocations?\n",
    "   - reference: https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.4.1. Get collocation\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# bigram association measures\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# construct bigrams using words from our example\n",
    "finder = BigramCollocationFinder.from_words(\\\n",
    "         nltk.regexp_tokenize(text, pattern))\n",
    "\n",
    "# find the top 10 bigrams by PMI \n",
    "# (Pointwise Mutual Information)\n",
    "# PMI(w1,w2)=log{p(w1,w2)/[p(w1)*p(w2)]}\n",
    "# due to small corpus, collocations may not be good\n",
    "finder.nbest(bigram_measures.pmi, 10)  \n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "#finder = BigramCollocationFinder.from_words(\\\n",
    "#        nltk.corpus.genesis.words('english-web.txt'))\n",
    "#finder.nbest(bigram_measures.pmi, 10)  \n",
    "#finder.nbest(bigram_measures.raw_freq, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS (Part of Speech) Tagging\n",
    "\n",
    " - What is POS Tagging:\n",
    "   * The process of marking up a word in a text as corresponding to a particular part of speech (e.g. nouns, verbs, adjectives, adverbs etc.), based on both **its definition**, as well as its **context** — adjacent and related words in a phrase, sentence, or paragraph. \n",
    " - Why POS Tagging: \n",
    "   * **disambiguation**: A word may have different meanings. POS tag is a potential strong signal for word sense disambiguation. For example, \"I fish a fish\"\n",
    "   * **Phrase extraction**: Use POS rules to define accepted phrases (or information unit) for indexing and retrieval:\n",
    "        * Adj + Noun, e.g. nice house\n",
    "        * Verb + Noun, e.g. play football\n",
    "   * **Filter tokens**:  some POS have less importance in retrieval, e.g. stopwords such as ‘a’, ‘an’, ‘the’, and other glue words like 'in', 'on', 'of' etc.\n",
    "   * Find other forms of a word based on POS\n",
    "        * Noun: plural and singular\n",
    "        * Verb: past, present and future tense\n",
    "        * Adjective: positive, comparative, and superlative\n",
    " - List of Penn Treebank Tags can be found at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    " - A tagger (program for tagging) is trained based on a corpus using machine learning approaches. It may not be very accurate when applying it your corpus.\n",
    "   - Stanford tagger (~97%)\n",
    "   - NLTK default tagger (PerceptronTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1. To find all tags in treebank\n",
    "nltk.help.upenn_tagset()\n",
    "\n",
    "# find the meaning of a specific tag\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.2. NLTK POS Tagging\n",
    "\n",
    "# The input to the tagging function is a list of words\n",
    "\n",
    "# tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# tag each tokenized word\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "\n",
    "print tagged_tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4.3. Extract Phrases by POS\n",
    "\n",
    "# Extract phrases in pattern (adjective noun)\n",
    "\n",
    "\n",
    "phrases=[ [tagged_tokens[i], tagged_tokens[i+1]] \\\n",
    "         for i in range(len(tagged_tokens)-1) \\\n",
    "         if tagged_tokens[i][1].startswith('JJ') and \\\n",
    "         # Tags for nouns can be NN, NNS, NNP, NNPS\n",
    "         tagged_tokens[i+1][1].startswith ('NN')]\n",
    "\n",
    "print(phrases)\n",
    "\n",
    "# how to extract phrase \"Noun + Verb\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization: Stemming & Lemmatization\n",
    " - What is normalization:\n",
    "   - Converts a list of words in **different surface forms** to a more **uniform form**, e.g.\n",
    "        * a word with different forms, e.g. organize, organizes, organized, and organizing\n",
    "        * families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n",
    " - Why normalization\n",
    "   - **improve text matching**: in many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "   - reduce featue space generated from text\n",
    " - Stemming and lemmatization are two common techinques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Stemming \n",
    "\n",
    "* **Stemming**: reducing inflected (or sometimes derived) words to their **stem, base or root** form. \n",
    "   * For example, **crying** -> **cri**. \n",
    "   * Stemming may not generate a real word, but a root form. \n",
    "   * The stemming program is called stemmer. \n",
    "       * Famous stemers are Porter stemmer, Lancaster Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.1.1. Stermming Using Porter Stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Stem of organizing/organized/organizes/organization\")\n",
    "print(porter_stemmer.stem('organizing'))\n",
    "print(porter_stemmer.stem('organized'))\n",
    "print(porter_stemmer.stem('organizes'))\n",
    "print(porter_stemmer.stem('organization'))\n",
    "\n",
    "print(\"\\nStem of crying\")\n",
    "print(porter_stemmer.stem('crying'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Lemmatization\n",
    "\n",
    "* **Lemmatization**: determining the lemma for a given word, \n",
    "   * A lemma is a word which stands at the head of a definition in a dictionary, e.g. run (lemma),  runs, ran and running (inflections) \n",
    "   * Lemmatization is a complex task involving understanding context and determining the part of speech of a word in a sentence \n",
    "      * e.g. \"organized\" (verb or adjective?)\n",
    "   * The widely used Lemmatization method is based on WordNet, a large lexical database of English.\n",
    "\n",
    "* **Difference** between stemming and lemmatization: \n",
    "   * a stemmer operates on a single word **without knowledge of the context**, and therefore cannot discriminate between words which have different meanings depending on part of speech. While, lemmatization **requires context and POS tags**. \n",
    "   * Stemming may not generate a real word, but lemmization always generates real words.\n",
    "   *  However, stemmers are typically easier to implement and run faster with reduced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organizing (verb) organize\n",
      "organized (verb) organize\n",
      "organized (adjective) organized\n",
      "organization (noun) organization\n",
      "crying (adjective) crying\n",
      "crying (verb) cry\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5.2.1. Lemmatization\n",
    "\n",
    "# wordnet lemmatizer takes POS tag as a parameter\n",
    "# The default POS tag is noun, \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"organizing (verb)\", \\\n",
    "      wordnet_lemmatizer.lemmatize('organizing', wordnet.VERB))\n",
    "print('organized (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize('organized', wordnet.VERB))\n",
    "print('organized (adjective)',\\\n",
    "      wordnet_lemmatizer.lemmatize('organized', wordnet.ADJ))\n",
    "print('organization (noun)',\\\n",
    "      wordnet_lemmatizer.lemmatize('organization'))\n",
    "print('crying (adjective)',\\\n",
    "      wordnet_lemmatizer.lemmatize('crying', wordnet.ADJ))\n",
    "print('crying (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize('crying', wordnet.VERB))\n",
    "\n",
    "# compare the result with Exercise 5.1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.2.2. Lemmatize the tokens from text \n",
    "          \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# first tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# then find the POS tag of each word\n",
    "# tagged_token is a list of (word, pos_tag)\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)\n",
    "\n",
    "# wordnet and treebank have different tagging systems\n",
    "# define a mapping between wordnet tags and POS tags as a function\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# get lemmatized tokens\n",
    "                            # lemmatize every word in tagged_tokens\n",
    "le_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "\n",
    "# get lemmatized unique tokens as vocabulary\n",
    "le_vocabulary=set(le_words)\n",
    "\n",
    "# count each lemmatized tokens\n",
    "le_word_dict={word: le_words.count(word) \\\n",
    "                    for word in le_words}\n",
    "\n",
    "print(\"\\nsort dictionary by frequency\")\n",
    "print(sorted(le_word_dict.items(), \\\n",
    "             key=lambda item:-item[1]))\n",
    "\n",
    "# notice rising/rose -> rise, prices -> price\n",
    "# notice the two \"end\" tokens are tagged as VBP and NN perspectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "- Definition: find and classify real word entities (Person, Organization, Event etc.) in text\n",
    "- Example: sentence \"Jim bought 300 shares of Acme Corp. in 2006\" can be annotated as \"**[Jim]<sub>Person</sub>** bought 300 shares of **[Acme Corp.]<sub>Organization</sub>** in 2006\"\n",
    "- Uses of NER:\n",
    "   *  Information Extraction: extract clear, factual information, i.e., Who did what to whom when?\n",
    "   *  Named entities can be indexed, and their relations can be extracted.\n",
    "   *  Sentiment can be attributed to companies or products\n",
    "   *  For question answering, answers are often named entities.\n",
    "- Techniques for NER\n",
    "   * Regular expression: Telephone numbers, emails, Capital names (e.g. Capitalized word + {city,  center, river}\n",
    "      * Adantages: simple and sometimes effective\n",
    "      * Disadvantage: \n",
    "         * first word of a sentence is capitalized; sometimes, titles are all capitalized; new proper names constantly emerges (e.g. movie titles, books, etc.)\n",
    "         * proper names may be ambiguous, e.g. Jordan can be *person* or *location*\n",
    "   * Supervised learning (IOB) (https://arxiv.org/abs/cmp-lg/9505040)\n",
    "       1. Collect a set of representative training documents\n",
    "       2. Label each token for its entity class (I: inside entity, B: begining entity) or other (O)\n",
    "       3. Design feature extractors appropriate to the text and classes, e.g. current word, pre/next word, pos tags etc.\n",
    "       4. Train a sequence classifier to predict the labels from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 6.1. Use NLTK for Named Entity Recognition\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "sentence = \"Jim bought 300 shares of Acme Corp. in 2006.\"\n",
    "\n",
    "# the input to ne_chunk is list of (token, pos tag) tuples\n",
    "ner_tree=ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# ne_chunk returns a tree\n",
    "print(ner_tree)\n",
    "\n",
    "# get PERSON out of the tree\n",
    "person=[]\n",
    "for t in ner_tree.subtrees():\n",
    "    if t.label() == 'PERSON':\n",
    "        person.append(t.leaves())\n",
    "print(\"PERSON\",person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    " - Motivation: How to identify important words (or phrases, named entities) in a text in a collecton or corpus? When search for documents, we'd like to have these important words are matched.\n",
    " - Intuition: \n",
    "   * In a document, if a word/term/phrase is repeated many times, it is likely important. \n",
    "   * However, if it appears in most of the documents in the corpus, then it has little discriminating power in determining relevance. \n",
    "   * For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. Search by \"auto\" you may get all the documents. \n",
    " - ** TF-IDF**: is composed by two terms: \n",
    "      - **TF(Term Frequency)**: which measures how frequently a term, say w, occurs in a document. \n",
    "      - **IDF (Inverse Document Frequency)**: measures how important a term is within the corpus. \n",
    " \n",
    " - TF_IDF provides another way to remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Term Frequency (TF)\n",
    "- Measures how frequently a term, say w, occurs in a document, say $d$. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. \n",
    "- Thus, the frequency of $w$ in $d$, denoted as $freq(w,d)$ is often divided by the document length (a.k.a. the total number of terms in the document, denoted as $|d|$) as a way of normalization: $$tf(w,d) = \\frac{freq(w,d)}{|d|}$$\n",
    "- Example: d=\"Stocks end up near year end\"\n",
    "   * tf('Stocks',d)=?\n",
    "   * tf('end',d)=?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Inverse Document Frequency (IDF)\n",
    "- Measures how important a term is within the corpus. While computing TF, all terms are considered equally important. \n",
    "- However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. \n",
    "- Thus we need to weigh down the frequent terms while scale up the rare ones. \n",
    "- Let $|D|$ denote the number of documents, $df(w,D)$ denotes the number of documents with term $w$ in them. Then, $$idf(w) = ln(\\frac{|D|}{df(w,D)})+1$$ Or a smoothed version: $$idf(t) = ln(\\frac{|D|+1}{df(w,D)+1})+1$$\n",
    "- Examples: \n",
    "  * Considering dataset:\n",
    "       1. \"Oil prices soar to all-time record\", \n",
    "       2. \"Stocks end up near year end\", \n",
    "       3. \"Money funds rose in latest week\",\n",
    "       4. \"Stocks up; traders eye crude oil prices\",\n",
    "       5. \"Dollar rising broadly on record trade gain\"\n",
    "  * idf('Stocks')=?\n",
    "  * idf('all-time')=?\n",
    "  * Discussion:\n",
    "     * What words get very low IDF score?\n",
    "     * What words get very high IDF score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. TF-IDF \n",
    "- Let $s(w,d)=tf(w,d) * idf(w)$, normalize the TF-IDF score of each word in a document normalized by the Euclidean norm, then \n",
    "   $$tfidf(w,d)=\\frac{s(w,d)}{\\sqrt{\\sum_{w \\in d}{s(w,d)^2}}}$$\n",
    "- For details of Euclidean norm (a.k.a L2 norm), see https://stanford.edu/class/ee103/lectures/norm/norm_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7.1. computing tf-idf\n",
    "\n",
    "\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# numpy is the package for matrix caculation\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "docs=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. get tokens of each document as list\n",
    "\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, or lemmatization here\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# step 2. process all documents to get list of token list\n",
    "docs_tokens=[get_doc_tokens(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 3. get document-term matrix\n",
    "# contruct a document-term matrix where \n",
    "# each row is a doc \n",
    "# each column is a token\n",
    "# and the value is the frequency of the token\n",
    "\n",
    "# first get unique tokens of all docs as vocabulary\n",
    "voc=list(set([token for tokens in docs_tokens \\\n",
    "              for token in tokens]))\n",
    "\n",
    "# initialize a matrix with all zeros\n",
    "# the size of the matrix is (len(docs), len(voc))\n",
    "dtm=np.zeros((len(docs), len(voc)))\n",
    "\n",
    "# loop through each doc\n",
    "for row_index,tokens in enumerate(docs_tokens):\n",
    "    # loop through each token\n",
    "    for token in tokens:\n",
    "        # find the column index of the token\n",
    "        col_index=voc.index(token)\n",
    "        # increase the cell value by 1\n",
    "        dtm[row_index, col_index]+=1\n",
    "        \n",
    "print(voc)\n",
    "print(dtm.shape)\n",
    "print(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Term Frequency Matrix **\n",
    "<img src='tf.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 4. get normalized term frequency (tf) matrix\n",
    "\n",
    "# sum the value of each row\n",
    "doc_len=dtm.sum(axis=1, keepdims=True)\n",
    "print(doc_len)\n",
    "\n",
    "# divide dtm matrix by the doc length matrix\n",
    "tf=np.divide(dtm, doc_len)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 5. get idf\n",
    "\n",
    "# get a copy of term frequency matrix\n",
    "# will create idf matrix based on dtm\n",
    "doc_freq=np.copy(dtm)\n",
    "\n",
    "# if a term frequency >0, set it to 1, \n",
    "# indicating the term appears in the doc\n",
    "# no matter how many times it appears\n",
    "doc_freq[np.where(doc_freq>0)]=1\n",
    "print (doc_freq)\n",
    "\n",
    "idf=np.log(np.divide(len(docs), np.sum(doc_freq, axis=0)))+1\n",
    "print(\"\\nIDF Matrix\")\n",
    "print (idf)\n",
    "\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(doc_freq, axis=0)+1))+1\n",
    "print(\"\\nSmoothed IDF Matrix\")\n",
    "print(smoothed_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 6. get tf-idf\n",
    "print(\"TF-IDF Matrix\")\n",
    "tf_idf=normalize(tf*idf)\n",
    "print(tf_idf)\n",
    "\n",
    "print(\"\\nSmoothed TF-IDF Matrix\")\n",
    "smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "print(smoothed_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF matrix gives **weight** of each work in each document\n",
    "- Documents:\n",
    "    1. \"Oil prices soar to all-time record\", \n",
    "    2. \"Stocks end up near year end\", \n",
    "    3. \"Money funds rose in latest week\",\n",
    "    4. \"Stocks up; traders eye crude oil prices\",\n",
    "    5. \"Dollar rising broadly on record trade gain\"\n",
    "<img src='tfidf.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. What to do with TF-IDF\n",
    "- This is the feature sapce of text mining (Bag of Words)\n",
    "- Find similar documents\n",
    "    * How to measure simialrity (or distance)? http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf\n",
    "    * Euclidean distance: Euclidean distance is **large** for vectors of high dimension (curse of dimensionality)\n",
    "    * Cosine similarity: The similarity between two documents is a function of the angle between their vectors in the if-idf vector space. \n",
    "      <img src='cosine.png' width=50% />\n",
    "      <img src='cosine_formula.svg' width=50% />\n",
    "      - Example: A=[0,2,1], B=[1,1,2], then\n",
    "      $$cosine(A,B)=\\frac{0*1+2*1+1*2}{\\sqrt{0+4+1}*\\sqrt{1+1+4}}$$\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exercise 7.4.1 Document similarity\n",
    "\n",
    "# package to calculate distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# convert the distance object into a matrix form\n",
    "# similarity is 1-distance\n",
    "similarity=1-distance.squareform(distance.pdist(tf_idf_matrix, 'cosine'))\n",
    "print similarity\n",
    "\n",
    "# get top docs similar to first doc \n",
    "doc_1_sin=similarity[0].tolist()  # convert first row to a list\n",
    "# sort \n",
    "print(sorted(enumerate(doc_1_sim), key=lambda item:-item[1])[0:2])\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(idx,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Put Everyting together -- Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "# numpy is the package for matrix cacluation\n",
    "import numpy as np  \n",
    "\n",
    "# Step 1. get tokens of each document as list\n",
    "def get_doc_tokens(doc):\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, or lemmatization here\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tfidf(docs):\n",
    "    # step 2. process all documents to get list of token list\n",
    "    docs_tokens=[get_doc_tokens(doc) for doc in docs]\n",
    "    voc=list(set([token for tokens in docs_tokens \\\n",
    "              for token in tokens]))\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "    dtm=np.zeros((len(docs), len(voc)))\n",
    "\n",
    "    for row_index,tokens in enumerate(docs_tokens):\n",
    "        for token in tokens:\n",
    "            col_index=voc.index(token)\n",
    "            dtm[row_index, col_index]+=1\n",
    "            \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    doc_len=dtm.sum(axis=1, keepdims=True)\n",
    "    tf=np.divide(dtm, doc_len)\n",
    "    \n",
    "    # step 5. get idf\n",
    "    doc_freq=np.copy(dtm)\n",
    "    doc_freq[np.where(doc_freq>0)]=1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(doc_freq, axis=0)+1))+1\n",
    "\n",
    "    \n",
    "    # step 6. get tf-idf\n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
