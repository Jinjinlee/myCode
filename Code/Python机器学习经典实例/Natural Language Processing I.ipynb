{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    " - https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    " - http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into tokens or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Create document-to-term matrix (bag of words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`strange days' chronicles the last two days of 1999 in los angeles . \n",
      " as the locals gear up for the new millenium , lenny nero ( ralph fiennes ) goes about his business of peddling erotic memory clips . \n",
      " he pines for his ex-girlfriend , faith ( juliette lewis ) , not noticing that another friend , mace ( angela bassett ) really cares for him . \n",
      " this film features good performances , impressive film-making technique and breath-taking crowd scenes . \n",
      " director kathryn bigelow knows her stuff and does not hesitate to use it . \n",
      " but as a whole , this is an unsatisfying movie . \n",
      " the problem is that the writers , james cameron and jay cocks , were too ambitious , aiming for a film with social relevance , thrills , and drama . \n",
      " not that ambitious film-making should be discouraged ; just that when it fails to achieve its goals , it fails badly and obviously . \n",
      " the film just ends up preachy , unexciting and uninvolving . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1. Load the text for analysis\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# load the document\n",
    "with open('movie_reivew.txt', 'r') as f:\n",
    "    lines=f.readlines()\n",
    "    \n",
    "# merge list into one string\n",
    "text=\" \".join(lines)\n",
    "\n",
    "print text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "['', 'strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex', 'girlfriend', 'faith', 'juliette', 'lewis', 'not', 'noticing', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film', 'making', 'technique', 'and', 'breath', 'taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film', 'making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving', '']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print len(tokens)                   \n",
    "print (tokens)                       \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: breath-taking and film-making \n",
    "# are split into two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "['`strange', 'days', \"'\", 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', '.', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', ',', 'lenny', 'nero', '(', 'ralph', 'fiennes', ')', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', '.', 'he', 'pines', 'for', 'his', 'ex-girlfriend', ',', 'faith', '(', 'juliette', 'lewis', ')', ',', 'not', 'noticing', 'that', 'another', 'friend', ',', 'mace', '(', 'angela', 'bassett', ')', 'really', 'cares', 'for', 'him', '.', 'this', 'film', 'features', 'good', 'performances', ',', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', '.', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', '.', 'but', 'as', 'a', 'whole', ',', 'this', 'is', 'an', 'unsatisfying', 'movie', '.', 'the', 'problem', 'is', 'that', 'the', 'writers', ',', 'james', 'cameron', 'and', 'jay', 'cocks', ',', 'were', 'too', 'ambitious', ',', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', ',', 'thrills', ',', 'and', 'drama', '.', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', ';', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', ',', 'it', 'fails', 'badly', 'and', 'obviously', '.', 'the', 'film', 'just', 'ends', 'up', 'preachy', ',', 'unexciting', 'and', 'uninvolving', '.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print len(tokens)                    \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. breath-taking and film-making each is captured as one word\n",
    "# Pros: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex-girlfriend', 'faith', 'juliette', 'lewis', 'not', 'noticing', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving']\n"
     ]
    }
   ],
   "source": [
    "# remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "tokens=[token.strip(string.punctuation) for token in tokens]\n",
    "\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens if token.strip()!='']\n",
    "print tokens  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "['strange', 'days', 'chronicles', 'the', 'last', 'two', 'days', 'of', '1999', 'in', 'los', 'angeles', 'as', 'the', 'locals', 'gear', 'up', 'for', 'the', 'new', 'millenium', 'lenny', 'nero', 'ralph', 'fiennes', 'goes', 'about', 'his', 'business', 'of', 'peddling', 'erotic', 'memory', 'clips', 'he', 'pines', 'for', 'his', 'ex-girlfriend', 'faith', 'juliette', 'lewis', 'not', 'noticing', 'that', 'another', 'friend', 'mace', 'angela', 'bassett', 'really', 'cares', 'for', 'him', 'this', 'film', 'features', 'good', 'performances', 'impressive', 'film-making', 'technique', 'and', 'breath-taking', 'crowd', 'scenes', 'director', 'kathryn', 'bigelow', 'knows', 'her', 'stuff', 'and', 'does', 'not', 'hesitate', 'to', 'use', 'it', 'but', 'as', 'a', 'whole', 'this', 'is', 'an', 'unsatisfying', 'movie', 'the', 'problem', 'is', 'that', 'the', 'writers', 'james', 'cameron', 'and', 'jay', 'cocks', 'were', 'too', 'ambitious', 'aiming', 'for', 'a', 'film', 'with', 'social', 'relevance', 'thrills', 'and', 'drama', 'not', 'that', 'ambitious', 'film-making', 'should', 'be', 'discouraged', 'just', 'that', 'when', 'it', 'fails', 'to', 'achieve', 'its', 'goals', 'it', 'fails', 'badly', 'and', 'obviously', 'the', 'film', 'just', 'ends', 'up', 'preachy', 'unexciting', 'and', 'uninvolving']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.2 NLTK's regular expression tokenizer (customizable)\n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as a sequence of word characters  \n",
    "# followed by optional word characters or \"-|.\" \n",
    "# e.g. film-making, L.A.\n",
    "\n",
    "pattern=r'\\w+[\\w\\-\\.]*'                        \n",
    "\n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print len(tokens)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2. Vocabulary \n",
    " - Vocabulary: the set of unique tokens  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word\n",
    "    * Value: count of the word\n",
    " - Find what words are frequently used (stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['just', 'lewis', 'cameron', 'its', 'cocks', 'bassett', 'jay', 'technique', 'breath-taking', 'should', 'to', 'writers', 'relevance', 'achieve', 'good', 'unsatisfying', 'noticing', 'not', 'aiming', 'him', 'impressive', 'james', 'millenium', 'this', 'stuff', 'unexciting', 'crowd', 'up', 'erotic', 'locals', 'really', 'fails', 'for', 'movie', 'ralph', 'does', 'goes', 'new', 'be', 'ends', 'business', 'drama', 'faith', 'about', 'last', 'of', 'days', 'angeles', 'peddling', 'social', 'whole', 'features', 'pines', 'another', 'ex-girlfriend', 'use', 'her', 'cares', 'two', 'angela', 'los', 'too', 'memory', 'friend', 'knows', 'that', 'but', 'juliette', 'goals', 'film-making', 'with', 'he', '1999', 'scenes', 'clips', 'were', 'problem', 'and', 'is', 'it', 'an', 'kathryn', 'as', 'his', 'nero', 'in', 'bigelow', 'thrills', 'film', 'gear', 'hesitate', 'when', 'strange', 'ambitious', 'uninvolving', 'badly', 'director', 'mace', 'fiennes', 'preachy', 'discouraged', 'a', 'lenny', 'performances', 'obviously', 'chronicles', 'the'])\n",
      "\n",
      "sort by word\n",
      "{'just': 2, 'lewis': 1, 'cameron': 1, 'its': 1, 'cocks': 1, 'bassett': 1, 'jay': 1, 'technique': 1, 'breath-taking': 1, 'should': 1, 'to': 2, 'strange': 1, 'relevance': 1, 'achieve': 1, 'his': 2, 'unsatisfying': 1, 'noticing': 1, 'not': 3, 'aiming': 1, 'him': 1, 'impressive': 1, 'james': 1, 'millenium': 1, 'this': 2, 'stuff': 1, 'unexciting': 1, 'crowd': 1, 'up': 2, 'erotic': 1, 'locals': 1, 'really': 1, 'fails': 2, 'for': 4, 'movie': 1, 'ralph': 1, 'does': 1, 'goes': 1, 'new': 1, 'be': 1, 'ends': 1, 'business': 1, 'faith': 1, 'about': 1, 'last': 1, 'of': 2, 'days': 2, 'drama': 1, 'peddling': 1, 'social': 1, 'whole': 1, 'features': 1, 'pines': 1, 'another': 1, 'ex-girlfriend': 1, 'use': 1, 'her': 1, 'cares': 1, 'two': 1, 'angela': 1, 'los': 1, 'too': 1, 'memory': 1, 'friend': 1, 'knows': 1, 'that': 4, 'but': 1, 'juliette': 1, 'goals': 1, 'film-making': 2, 'with': 1, 'he': 1, '1999': 1, 'scenes': 1, 'clips': 1, 'were': 1, 'problem': 1, 'and': 6, 'is': 2, 'it': 3, 'an': 1, 'kathryn': 1, 'as': 2, 'good': 1, 'nero': 1, 'in': 1, 'bigelow': 1, 'thrills': 1, 'film': 3, 'gear': 1, 'angeles': 1, 'when': 1, 'writers': 1, 'ambitious': 2, 'uninvolving': 1, 'badly': 1, 'hesitate': 1, 'director': 1, 'mace': 1, 'fiennes': 1, 'preachy': 1, 'discouraged': 1, 'a': 2, 'lenny': 1, 'performances': 1, 'obviously': 1, 'chronicles': 1, 'the': 6}\n",
      "\n",
      "sort by frequency\n",
      "[('and', 6), ('the', 6), ('for', 4), ('that', 4), ('not', 3), ('it', 3), ('film', 3), ('just', 2), ('to', 2), ('his', 2), ('this', 2), ('up', 2), ('fails', 2), ('of', 2), ('days', 2), ('film-making', 2), ('is', 2), ('as', 2), ('ambitious', 2), ('a', 2), ('lewis', 1), ('cameron', 1), ('its', 1), ('cocks', 1), ('bassett', 1), ('jay', 1), ('technique', 1), ('breath-taking', 1), ('should', 1), ('strange', 1), ('relevance', 1), ('achieve', 1), ('unsatisfying', 1), ('noticing', 1), ('aiming', 1), ('him', 1), ('impressive', 1), ('james', 1), ('millenium', 1), ('stuff', 1), ('unexciting', 1), ('crowd', 1), ('erotic', 1), ('locals', 1), ('really', 1), ('movie', 1), ('ralph', 1), ('does', 1), ('goes', 1), ('new', 1), ('be', 1), ('ends', 1), ('business', 1), ('faith', 1), ('about', 1), ('last', 1), ('drama', 1), ('peddling', 1), ('social', 1), ('whole', 1), ('features', 1), ('pines', 1), ('another', 1), ('ex-girlfriend', 1), ('use', 1), ('her', 1), ('cares', 1), ('two', 1), ('angela', 1), ('los', 1), ('too', 1), ('memory', 1), ('friend', 1), ('knows', 1), ('but', 1), ('juliette', 1), ('goals', 1), ('with', 1), ('he', 1), ('1999', 1), ('scenes', 1), ('clips', 1), ('were', 1), ('problem', 1), ('an', 1), ('kathryn', 1), ('good', 1), ('nero', 1), ('in', 1), ('bigelow', 1), ('thrills', 1), ('gear', 1), ('angeles', 1), ('when', 1), ('writers', 1), ('uninvolving', 1), ('badly', 1), ('hesitate', 1), ('director', 1), ('mace', 1), ('fiennes', 1), ('preachy', 1), ('discouraged', 1), ('lenny', 1), ('performances', 1), ('obviously', 1), ('chronicles', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2.1 \n",
    "# Get vocabulary and dictionary of text\n",
    "\n",
    "vocabulary= set(tokens)                                        \n",
    "# set() convert a list to a set without any duplicates\n",
    "print (vocabulary)\n",
    "\n",
    "# tokens.count(word) returns the count of the word in tokens (list)\n",
    "dictionary={word: tokens.count(word) for word in vocabulary}\n",
    "# by default, dictionary is sorted by key\n",
    "print(\"\\nsort by word\")\n",
    "print (dictionary)\n",
    "\n",
    "# find what are the frequent words\n",
    "# sort the dictionary by value\n",
    "# sorted(iterable, key) sorts an iterable object by the comparison key\n",
    "# lambda: anonymous function defined without a name. \n",
    "# lambda item:-item[1] sorts the list by the 2nd element in a descending order\n",
    "print(\"\\nsort by frequency\")\n",
    "print(sorted(dictionary.items(), key=lambda item:-item[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.3. Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. You can define your own stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn', 'film', 'films']\n",
      "\n",
      "sort dictionary without stop words by frequency\n",
      "[('ambitious', 2), ('fails', 2), ('film-making', 2), ('days', 2), ('cares', 1), ('features', 1), ('crowd', 1), ('pines', 1), ('lewis', 1), ('uninvolving', 1), ('writers', 1), ('scenes', 1), ('kathryn', 1), ('bigelow', 1), ('erotic', 1), ('angela', 1), ('another', 1), ('ex-girlfriend', 1), ('cameron', 1), ('nero', 1), ('thrills', 1), ('locals', 1), ('really', 1), ('bassett', 1), ('friend', 1), ('discouraged', 1), ('jay', 1), ('movie', 1), ('technique', 1), ('ralph', 1), ('two', 1), ('breath-taking', 1), ('los', 1), ('strange', 1), ('obviously', 1), ('cocks', 1), ('goes', 1), ('memory', 1), ('relevance', 1), ('new', 1), ('performances', 1), ('achieve', 1), ('director', 1), ('badly', 1), ('ends', 1), ('good', 1), ('unsatisfying', 1), ('knows', 1), ('business', 1), ('hesitate', 1), ('use', 1), ('noticing', 1), ('juliette', 1), ('mace', 1), ('fiennes', 1), ('angeles', 1), ('goals', 1), ('aiming', 1), ('faith', 1), ('last', 1), ('impressive', 1), ('lenny', 1), ('unexciting', 1), ('james', 1), ('millenium', 1), ('clips', 1), ('drama', 1), ('1999', 1), ('stuff', 1), ('chronicles', 1), ('peddling', 1), ('gear', 1), ('social', 1), ('problem', 1), ('whole', 1), ('preachy', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"film\", \"films\"]\n",
    "print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dictionary={word: dictionary[word] for word in dictionary if word not in stop_words}\n",
    "print(\"\\nsort dictionary without stop words by frequency\")\n",
    "print(sorted(filtered_dictionary.items(), key=lambda item:-item[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['faith', 'good', 'impressive', 'ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3.2\n",
    "# Find positive words \n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "    \n",
    "positive_tokens=[token for token in tokens if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
